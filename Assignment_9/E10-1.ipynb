{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E10-1\n",
    "#### This Notebook illustrates the use of \"MAP-REDUCE\" to calculate averages from the data contained in nsedata.csv.\n",
    "\n",
    "### <b>Task 1</b>\n",
    "You are required to review the code (refer to the SPARK document where necessary), and <b>add comments / markup explaining the code in each cell</b>. Also explain the role of each cell in the overall context of the solution to the problem (ie. what is the cell trying to achieve in the overall scheme of things). You may create additional code in each cell to generate any debug output that you may need to complete this exercise.\n",
    "### <b>Task 2</b>\n",
    "You are required to write code to solve the problem stated at the end this Notebook\n",
    "### <b>Submission</b>\n",
    "Create and upload a PDF of this Notebook. <b> BEFORE CONVERTING TO PDF and UPLOADING ENSURE THAT YOU REMOVE / TRIM LENGTHY DEBUG OUTPUTS </b>. Short debug outputs of up to 5 lines are acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Line1:\n",
    "# Here we are importing the library findspark of Python which will help you to locate and use  PySpark installed in your system\n",
    "# Line 2:\n",
    "# We are using the function init from the library findspark.The function helps you to fing=d the location in which Pyspark is installed in your machine \n",
    "# and also initialises it to work with the current Python environment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "#Line 1:\n",
    "#Here we are importing the pyspark library of Python \n",
    "#Line 2:\n",
    "#Here we are accessing all the modules of types from sql from the bigger library PySpark.This module helps us to use different data types while \n",
    "#using different DataFrames in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/31 13:28:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/31 13:28:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"E10\")\n",
    "#Here we are setting up a Spark Context with the Application name E10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"/home/hduser/spark/nsedata.csv\")\n",
    "#Here we are creating a RDD file. RDDs are the fundamental data structure in Apache Spark, and they allow for distributed data processing.\n",
    "#We are reading the text file from the folder mentioned in the code above and storing it to rdd1 as a RDD file\n",
    "#Each line from the text file becomes an element in the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd1.filter(lambda x: \"SYMBOL\" not in x)\n",
    "#Here we are using the Lambda function and filter transformation to remove all the elements of the RDD with the word \"SYMBOL\" in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x : x.split(\",\"))\n",
    "#In this code given above,we are using the map transformation and Lambda function for splittting the elements of the RDD using the delimiter \",\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper comment!: The goal is to find out the mean of the OPEN prices and the mean of the CLOSE price in one batch of tasks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_open = rdd2.map(lambda x : (x[0]+\"_open\",float(x[2])))\n",
    "rdd_close = rdd2.map(lambda x : (x[0]+\"_close\",float(x[5])))\n",
    "#We are creating a key-value pair by using Lambda and map transformation .In rdd_open,the key is formed by concatenating x[0] with _open \n",
    "#and in rdd_close we are concatenating x[0] with _close for making the key.For the values we are adding the corresponding \n",
    "#open and close values from the csv file using x[2] and x[5] in float form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_united = rdd_open.union(rdd_close)\n",
    "#Here we are using the Union transformation to combine both rdd_open and rdd_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedByKey = rdd_united.reduceByKey(lambda x,y: x+y)\n",
    "#We are using reduceByKey transformation to combine the values with same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp1 = rdd_united.map(lambda x: (x[0],1)).countByKey()\n",
    "countOfEachSymbol = sc.parallelize(temp1.items())\n",
    "# It maps each element (key-value pair) to a new key-value pair where the key is the stock symbol x[0] and set the value to 1.\n",
    "#The above step effectively converts the data to a format where each symbol is associated with the value 1.\n",
    "#sc.parallelize(temp1.items()) is used to convert the dictionary items into an RDD.\n",
    "#Each of the key-value pairs represent a stock symbol and its corresponding count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_sum_count = reducedByKey.join(countOfEachSymbol)\n",
    "#Here we are joining the two RDD's reducedByKey and countOfEachSymbol,based on their keys by using the join transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = symbol_sum_count.map(lambda x : (x[0], x[1][0]/x[1][1]))\n",
    "#Here we are using the map transformation on the symbol_sum_count RDD to calculate the averages by dividing\n",
    "#the sum of open and close prices by the count of occurrences for each stock symbol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "averagesSorted = averages.sortByKey()\n",
    "#Here we are using the sortByKey transformation to sort the averages RDD by its keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "averagesSorted.saveAsTextFile(\"/home/hduser/spark/averages\")\n",
    "#Here we are using the saveAsTextFile action to save the averagesSorted RDD to a text file to the destination mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the output files generated in the above step and copy the first 15 lines of any one of the output files into the cell below for reference. Write your comments on the generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20MICRONS_close', 53.004122877930484)\n",
      "\n",
      "('20MICRONS_open', 53.32489894907032)\n",
      "\n",
      "('3IINFOTECH_close', 18.038803556992725)\n",
      "\n",
      "('3IINFOTECH_open', 18.17417138237672)\n",
      "\n",
      "('3MINDIA_close', 4520.343977364591)\n",
      "\n",
      "('3MINDIA_open', 4531.084518997574)\n",
      "\n",
      "('3RDROCK_close', 173.2137755102041)\n",
      "\n",
      "('3RDROCK_open', 173.18316326530612)\n",
      "\n",
      "('8KMILES_close', 480.73622047244095)\n",
      "\n",
      "('8KMILES_open', 481.63858267716535)\n",
      "\n",
      "('A2ZINFRA_close', 18.609433962264156)\n",
      "\n",
      "('A2ZINFRA_open', 18.73553459119497)\n",
      "\n",
      "('A2ZMES_close', 89.69389505549951)\n",
      "\n",
      "('A2ZMES_open', 90.46271442986883)\n",
      "\n",
      "('AANJANEYA_close', 441.84030249110316)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/hduser/spark/averages/part-00000\", \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i < 15:\n",
    "            print(line)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "#The below listed are the first 15 lines from part 00000 of the saved file \n",
    "#As we can see,we can easily get to know the average profit/loss made by trading the given stock from the given average values of opening and closing \n",
    "#stock price of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Problem Statement\n",
    "### Using the MAP-REDUCE strategy, write SPARK code that will create the average of HIGH prices for all the traded companies, but only for any 3 months of your choice. Create the appropriate (K,V) pairs so that the averages are simultaneously calculated, as in the above example. Create the output files such that the final data is sorted in <b>descending order</b> of the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.textFile(\"/home/hduser/spark/nsedata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=rdd1.filter(lambda x:\"SYMBOL\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd1.map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20MICRONS_high_average', 37.75)\n",
      "('3IINFOTECH_high_average', 45.3)\n",
      "('3MINDIA_high_average', 3439.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_high=rdd2.map(lambda x: (x[0]+\"_high_average\",float(x[3]))) \n",
    "elements = rdd_high.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===========================================================(5 + 0) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AARTIDRUGS_high_average', 396832.7999999998)\n",
      "('ABB_high_average', 1063958.7500000002)\n",
      "('ABBOTINDIA_high_average', 2425757.700000001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reducedByKey_2 = rdd_high.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "elements = reducedByKey_2.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20MICRONS_high_average', 1237)\n",
      "('3IINFOTECH_high_average', 1237)\n",
      "('3MINDIA_high_average', 1237)\n"
     ]
    }
   ],
   "source": [
    "temp1_2 = rdd_high.map(lambda x: (x[0],1)).countByKey()\n",
    "countOfEachSymbol_2 = sc.parallelize(temp1_2.items())\n",
    "elements = countOfEachSymbol_2.take(3)\n",
    "\n",
    "# Print or examine the elements\n",
    "for element in elements :\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20MICRONS_high_average', (67564.34999999998, 1237))\n",
      "('3IINFOTECH_high_average', (22960.199999999997, 1237))\n",
      "('3MINDIA_high_average', (5694089.6499999985, 1237))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "symbol_sum_count_2 = reducedByKey_2.join(countOfEachSymbol_2)\n",
    "temporary_2 = symbol_sum_count_2.sortByKey()\n",
    "# elements = symbol_sum_count_2.take(3)\n",
    "# # Print or examine the elements\n",
    "# for element in elements:\n",
    "#     print(element)\n",
    "\n",
    "elements = temporary_2.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ABBOTINDIA_high_average', 1961.0005658852072)\n",
      "('ACC_high_average', 1257.7121665319323)\n",
      "('ANGIND_high_average', 26.880166821130672)\n"
     ]
    }
   ],
   "source": [
    "averages_2 = symbol_sum_count_2.map(lambda x : (x[0], x[1][0]/x[1][1]))\n",
    "\n",
    "elements = averages_2.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20MICRONS_high_average', 54.61952303961195)\n",
      "('3IINFOTECH_high_average', 18.561196443007272)\n",
      "('3MINDIA_high_average', 4603.144421988681)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "averagesSorted_2 = averages_2.sortByKey()\n",
    "# Assuming you have an RDD named 'my_rdd'\n",
    "elements = averagesSorted_2.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "averagesSorted_2.saveAsTextFile(\"/home/hduser/spark/averages_high_all_months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above file contains the average of high values of all the companies from all the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2=sc.textFile(\"/home/hduser/spark/nsedata.csv\")\n",
    "rdd_2=rdd_2.filter(lambda x:\"SYMBOL\" not in x)\n",
    "# rdd_high_2=rdd_21.map(lambda x: (x[0]+\"_high\",float(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ABAN', 10900.05)\n",
      "('ABGSHIP', 4123.4)\n",
      "('ACE', 581.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_sample=rdd_2.filter(lambda x:(\"OCT-2014\" or \"NOV-2014\" or \"DEC-2014\") in x)\n",
    "temp=temp_sample.map(lambda x:x.split(\",\"))\n",
    "temp_high=temp.map(lambda x : (x[0],float(x[3])))\n",
    "temp_by_key=temp_high.reduceByKey(lambda x,y : x+y)\n",
    "\n",
    "elements = temp_by_key.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_2 = temp_by_key.map(lambda x : (x[0],1)).countByKey()\n",
    "counts = sc.parallelize(temp_2.items())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "symbol_highsum_count = temp_by_key.join(counts)\n",
    "avg_high = symbol_highsum_count.map(lambda x : (x[0] , x[1][0]/x[1][1]))\n",
    "avgs_desc = avg_high.sortByKey(False)\n",
    "#Setting the sortByKey to false will sort it by keys in the descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=================================>                        (4 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ZYLOG', 140.15)\n",
      "('ZYDUSWELL', 11556.099999999999)\n",
      "('ZUARIGLOB', 1681.6000000000001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "elements = avgs_desc.take(3)\n",
    "# Print or examine the elements\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avgs_desc.saveAsTextFile(\"/home/hduser/spark/average_highest_3months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
